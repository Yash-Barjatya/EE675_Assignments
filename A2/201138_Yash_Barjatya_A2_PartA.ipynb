{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2A\n",
    "\n",
    "**Name**: Yash Barjatya                              </br>\n",
    "**Roll No.**:  201138"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "import gymnasium as gym\n",
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager\n",
    "import random\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Represents a Stochastic Maze problem Gym Environment which provides a Fully observable\n",
    "MDP\n",
    "'''\n",
    "class StochasticMazeEnv(gym.Env):\n",
    "    '''\n",
    "    StochasticMazeEnv represents the Gym Environment for the Stochastic Maze environment\n",
    "    States : [0,1,2,3,4,5,6,7,8,9,10,11]\n",
    "    Actions : [\"Left\":0, \"Up\":1, \"Right\":2, \"Down\":3]\n",
    "    '''\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self,initial_state=0,no_states=12,no_actions=4):\n",
    "        '''\n",
    "        Constructor for the StochasticMazeEnv class\n",
    "\n",
    "        Args:\n",
    "            initial_state : starting state of the agent\n",
    "            no_states : The no. of possible states which is 12\n",
    "            no_actions : The no. of possible actions which is 4\n",
    "            \n",
    "        '''\n",
    "        self.initial_state = initial_state\n",
    "        self.state = self.initial_state\n",
    "        self.nA = no_actions\n",
    "        self.nS = no_states\n",
    "        self.actions_dict = {\"L\":0, \"U\":1, \"R\":2, \"D\":3}\n",
    "        self.prob_dynamics = {\n",
    "            0: {\n",
    "                0: [(0.8, 0, -0.01, False), (0.1, 0, -0.01, False), (0.1, 4, -0.01, False)],\n",
    "                1: [(0.8, 0, -0.01, False), (0.1, 0, -0.01, False), (0.1, 1, -0.01, False)],\n",
    "                2: [(0.8, 1, -0.01, False), (0.1, 0, -0.01, False), (0.1, 4, -0.01, False)],\n",
    "                3: [(0.8, 4, -0.01, False), (0.1, 0, -0.01, False), (0.1, 1, -0.01, False)],\n",
    "            },\n",
    "            1: {\n",
    "                0: [(0.8, 0, -0.01, False), (0.1, 1, -0.01, False), (0.1, 1, -0.01, False)],\n",
    "                1: [(0.8, 1, -0.01, False), (0.1, 0, -0.01, False), (0.1, 2, -0.01, False)],\n",
    "                2: [(0.8, 2, -0.01, False), (0.1, 1, -0.01, False), (0.1, 1, -0.01, False)],\n",
    "                3: [(0.8, 1, -0.01, False), (0.1, 0, -0.01, False), (0.1, 2, -0.01, False)],\n",
    "            },\n",
    "            2: {\n",
    "                0: [(0.8, 1, -0.01, False), (0.1, 2, -0.01, False), (0.1, 6, -0.01, False)],\n",
    "                1: [(0.8, 2, -0.01, False), (0.1, 1, -0.01, False), (0.1, 3, +1, True)],\n",
    "                2: [(0.8, 3, +1, True), (0.1, 2, -0.01, False), (0.1, 6, -0.01, False)],\n",
    "                3: [(0.8, 6, -0.01, False), (0.1, 1, -0.01, False), (0.1, 3, +1, True)],\n",
    "            },\n",
    "            3: {\n",
    "                0: [(0.8, 3, 0, True), (0.1, 3, 0, True), (0.1, 3, 0, True)],\n",
    "                1: [(0.8, 3, 0, True), (0.1, 3, 0, True), (0.1, 3, 0, True)],\n",
    "                2: [(0.8, 3, 0, True), (0.1, 3, 0, True), (0.1, 3, 0, True)],\n",
    "                3: [(0.8, 3, 0, True), (0.1, 3, 0, True), (0.1, 3, 0, True)],\n",
    "            },\n",
    "            4: {\n",
    "                0: [(0.8, 4, -0.01, False), (0.1, 0, -0.01, False), (0.1, 8, -0.01, False)],\n",
    "                1: [(0.8, 0, -0.01, False), (0.1, 4, -0.01, False), (0.1, 4, -0.01, False)],\n",
    "                2: [(0.8, 4, -0.01, False), (0.1, 0, -0.01, False), (0.1, 8, -0.01, False)],\n",
    "                3: [(0.8, 8, -0.01, False), (0.1, 4, -0.01, False), (0.1, 4, -0.01, False)],\n",
    "            },\n",
    "            6: {\n",
    "                0: [(0.8, 6, -0.01, False), (0.1, 2, -0.01, False), (0.1, 10, -0.01, False)],\n",
    "                1: [(0.8, 2, -0.01, False), (0.1, 6, -0.01, False), (0.1, 7, -1, True)],\n",
    "                2: [(0.8, 7, -1, True), (0.1, 2, -0.01, False), (0.1, 10, -0.01, False)],\n",
    "                3: [(0.8, 10, -0.01, False), (0.1, 6, -0.01, False), (0.1, 7, -1, True)],\n",
    "            },\n",
    "            7: {\n",
    "                0: [(0.8, 7, 0, True), (0.1, 7, 0, True), (0.1, 7, 0, True)],\n",
    "                1: [(0.8, 7, 0, True), (0.1, 7, 0, True), (0.1, 7, 0, True)],\n",
    "                2: [(0.8, 7, 0, True), (0.1, 7, 0, True), (0.1, 7, 0, True)],\n",
    "                3: [(0.8, 7, 0, True), (0.1, 7, 0, True), (0.1, 7, 0, True)],\n",
    "            },\n",
    "            8: {\n",
    "                0: [(0.8, 8, -0.01, False), (0.1, 8, -0.01, False), (0.1, 4, -0.01, False)],\n",
    "                1: [(0.8, 4, -0.01, False), (0.1, 8, -0.01, False), (0.1, 9, -0.01, False)],\n",
    "                2: [(0.8, 9, -0.01, False), (0.1, 8, -0.01, False), (0.1, 4, -0.01, False)],\n",
    "                3: [(0.8, 8, -0.01, False), (0.1, 8, -0.01, False), (0.1, 9, -0.01, False)],\n",
    "            },\n",
    "            9: {\n",
    "                0: [(0.8, 8, -0.01, False), (0.1, 9, -0.01, False), (0.1, 9, -0.01, False)],\n",
    "                1: [(0.8, 9, -0.01, False), (0.1, 8, -0.01, False), (0.1, 10, -0.01, False)],\n",
    "                2: [(0.8, 10, -0.01, False), (0.1, 9, -0.01, False), (0.1, 9, -0.01, False)],\n",
    "                3: [(0.8, 9, -0.01, False), (0.1, 8, -0.01, False), (0.1, 10, -0.01, False)]\n",
    "            },\n",
    "            10: {\n",
    "                0: [(0.8, 9, -0.01, False), (0.1, 6, -0.01, False), (0.1, 10, -0.01, False)],\n",
    "                1: [(0.8, 6, -0.01, False), (0.1, 9, -0.01, False), (0.1, 11, -0.01, False)],\n",
    "                2: [(0.8, 11, -0.01, False), (0.1, 6, -0.01, False), (0.1, 10, -0.01, False)],\n",
    "                3: [(0.8, 10, -0.01, False), (0.1, 9, -0.01, False), (0.1, 11, -0.01, False)]\n",
    "            },\n",
    "            11: {\n",
    "                0: [(0.8, 10, -0.01, False), (0.1, 7, -1, True), (0.1, 11, -0.01, False)],\n",
    "                1: [(0.8, 7, -1, True), (0.1, 10, -0.01, False), (0.1, 11, -0.01, False)],\n",
    "                2: [(0.8, 11, -0.01, False), (0.1, 7, -1, True), (0.1, 11, -0.01, False)],\n",
    "                3: [(0.8, 11, -0.01, False), (0.1, 11, -0.01, False), (0.1, 10, -0.01, False)]\n",
    "            }\n",
    "        }\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        '''\n",
    "        Resets the environment\n",
    "        Returns:\n",
    "            observations containing player's current state\n",
    "        '''\n",
    "        self.state = self.initial_state\n",
    "        return self.get_obs()\n",
    "\n",
    "    def get_obs(self):\n",
    "        '''\n",
    "        Returns the player's state as the observation of the environment\n",
    "        '''\n",
    "        return (self.state)\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        '''\n",
    "        Renders the environment\n",
    "        '''\n",
    "        print(\"Current state: {}\".format(self.state))\n",
    "\n",
    "    def sample_action(self):\n",
    "        '''\n",
    "        Samples and returns a random action from the action space\n",
    "        '''\n",
    "        return random.randint(0, self.nA)\n",
    "    def P(self):\n",
    "        '''\n",
    "        Defines and returns the probabilty transition matrix which is in the form of a nested dictionary\n",
    "        '''\n",
    "        self.prob_dynamics = {\n",
    "            0: {\n",
    "                0: [(0.8, 0, -0.01, False), (0.1, 0, -0.01, False), (0.1, 4, -0.01, False)],\n",
    "                1: [(0.8, 0, -0.01, False), (0.1, 0, -0.01, False), (0.1, 1, -0.01, False)],\n",
    "                2: [(0.8, 1, -0.01, False), (0.1, 0, -0.01, False), (0.1, 4, -0.01, False)],\n",
    "                3: [(0.8, 4, -0.01, False), (0.1, 0, -0.01, False), (0.1, 1, -0.01, False)],\n",
    "            },\n",
    "            1: {\n",
    "                0: [(0.8, 0, -0.01, False), (0.1, 1, -0.01, False), (0.1, 1, -0.01, False)],\n",
    "                1: [(0.8, 1, -0.01, False), (0.1, 0, -0.01, False), (0.1, 2, -0.01, False)],\n",
    "                2: [(0.8, 2, -0.01, False), (0.1, 1, -0.01, False), (0.1, 1, -0.01, False)],\n",
    "                3: [(0.8, 1, -0.01, False), (0.1, 0, -0.01, False), (0.1, 2, -0.01, False)],\n",
    "            },\n",
    "            2: {\n",
    "                0: [(0.8, 1, -0.01, False), (0.1, 2, -0.01, False), (0.1, 6, -0.01, False)],\n",
    "                1: [(0.8, 2, -0.01, False), (0.1, 1, -0.01, False), (0.1, 3, +1, True)],\n",
    "                2: [(0.8, 3, +1, True), (0.1, 2, -0.01, False), (0.1, 6, -0.01, False)],\n",
    "                3: [(0.8, 6, -0.01, False), (0.1, 1, -0.01, False), (0.1, 3, +1, True)],\n",
    "            },\n",
    "            3: {\n",
    "                0: [(0.8, 3, 0, True), (0.1, 3, 0, True), (0.1, 3, 0, True)],\n",
    "                1: [(0.8, 3, 0, True), (0.1, 3, 0, True), (0.1, 3, 0, True)],\n",
    "                2: [(0.8, 3, 0, True), (0.1, 3, 0, True), (0.1, 3, 0, True)],\n",
    "                3: [(0.8, 3, 0, True), (0.1, 3, 0, True), (0.1, 3, 0, True)],\n",
    "            },\n",
    "            4: {\n",
    "                0: [(0.8, 4, -0.01, False), (0.1, 0, -0.01, False), (0.1, 8, -0.01, False)],\n",
    "                1: [(0.8, 0, -0.01, False), (0.1, 4, -0.01, False), (0.1, 4, -0.01, False)],\n",
    "                2: [(0.8, 4, -0.01, False), (0.1, 0, -0.01, False), (0.1, 8, -0.01, False)],\n",
    "                3: [(0.8, 8, -0.01, False), (0.1, 4, -0.01, False), (0.1, 4, -0.01, False)],\n",
    "            },\n",
    "            6: {\n",
    "                0: [(0.8, 6, -0.01, False), (0.1, 2, -0.01, False), (0.1, 10, -0.01, False)],\n",
    "                1: [(0.8, 2, -0.01, False), (0.1, 6, -0.01, False), (0.1, 7, -1, True)],\n",
    "                2: [(0.8, 7, -1, True), (0.1, 2, -0.01, False), (0.1, 10, -0.01, False)],\n",
    "                3: [(0.8, 10, -0.01, False), (0.1, 6, -0.01, False), (0.1, 7, -1, True)],\n",
    "            },\n",
    "            7: {\n",
    "                0: [(0.8, 7, 0, True), (0.1, 7, 0, True), (0.1, 7, 0, True)],\n",
    "                1: [(0.8, 7, 0, True), (0.1, 7, 0, True), (0.1, 7, 0, True)],\n",
    "                2: [(0.8, 7, 0, True), (0.1, 7, 0, True), (0.1, 7, 0, True)],\n",
    "                3: [(0.8, 7, 0, True), (0.1, 7, 0, True), (0.1, 7, 0, True)],\n",
    "            },\n",
    "            8: {\n",
    "                0: [(0.8, 8, -0.01, False), (0.1, 8, -0.01, False), (0.1, 4, -0.01, False)],\n",
    "                1: [(0.8, 4, -0.01, False), (0.1, 8, -0.01, False), (0.1, 9, -0.01, False)],\n",
    "                2: [(0.8, 9, -0.01, False), (0.1, 8, -0.01, False), (0.1, 4, -0.01, False)],\n",
    "                3: [(0.8, 8, -0.01, False), (0.1, 8, -0.01, False), (0.1, 9, -0.01, False)],\n",
    "            },\n",
    "            9: {\n",
    "                0: [(0.8, 8, -0.01, False), (0.1, 9, -0.01, False), (0.1, 9, -0.01, False)],\n",
    "                1: [(0.8, 9, -0.01, False), (0.1, 8, -0.01, False), (0.1, 10, -0.01, False)],\n",
    "                2: [(0.8, 10, -0.01, False), (0.1, 9, -0.01, False), (0.1, 9, -0.01, False)],\n",
    "                3: [(0.8, 9, -0.01, False), (0.1, 8, -0.01, False), (0.1, 10, -0.01, False)]\n",
    "            },\n",
    "            10: {\n",
    "                0: [(0.8, 9, -0.01, False), (0.1, 6, -0.01, False), (0.1, 10, -0.01, False)],\n",
    "                1: [(0.8, 6, -0.01, False), (0.1, 9, -0.01, False), (0.1, 11, -0.01, False)],\n",
    "                2: [(0.8, 11, -0.01, False), (0.1, 6, -0.01, False), (0.1, 10, -0.01, False)],\n",
    "                3: [(0.8, 10, -0.01, False), (0.1, 9, -0.01, False), (0.1, 11, -0.01, False)]\n",
    "            },\n",
    "            11: {\n",
    "                0: [(0.8, 10, -0.01, False), (0.1, 7, -1, True), (0.1, 11, -0.01, False)],\n",
    "                1: [(0.8, 7, -1, True), (0.1, 10, -0.01, False), (0.1, 11, -0.01, False)],\n",
    "                2: [(0.8, 11, -0.01, False), (0.1, 7, -1, True), (0.1, 11, -0.01, False)],\n",
    "                3: [(0.8, 11, -0.01, False), (0.1, 11, -0.01, False), (0.1, 10, -0.01, False)]\n",
    "            }\n",
    "        }\n",
    "        return self.prob_dynamics\n",
    "\n",
    "    \n",
    "    def step(self, action):\n",
    "        '''\n",
    "        Performs the given action\n",
    "        Args:\n",
    "            action : action from the action_space to be taking in the environment\n",
    "        Returns:\n",
    "            observation - returns current state\n",
    "            reward - reward obtained after taking the given action\n",
    "            done - True if the episode is complete else False\n",
    "        '''\n",
    "        if action >= self.nA:\n",
    "            action = self.nA-1\n",
    "\n",
    "        index = np.random.choice(3,1,p=[0.8,0.1,0.1])[0]\n",
    "\n",
    "        dynamics_tuple = self.prob_dynamics[self.state][action][index]\n",
    "        self.state = dynamics_tuple[1]\n",
    "        \n",
    "\n",
    "        return self.state, dynamics_tuple[2], dynamics_tuple[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = StochasticMazeEnv()\n",
    "env.reset()\n",
    "num_states = env.nS\n",
    "num_actions = env.nA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Cases for checking the environment "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Random Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action\t State\t Reward\t is_Terminal\n",
      "   2 \t   1 \t -0.01 \t   False\n",
      "   0 \t   1 \t -0.01 \t   False\n",
      "   1 \t   1 \t -0.01 \t   False\n",
      "   3 \t   1 \t -0.01 \t   False\n",
      "   3 \t   1 \t -0.01 \t   False\n",
      "   0 \t   0 \t -0.01 \t   False\n",
      "   2 \t   1 \t -0.01 \t   False\n",
      "   2 \t   2 \t -0.01 \t   False\n",
      "   2 \t   3 \t 1 \t   True\n",
      "Total Number of steps to Reach Terminal: 9\n",
      "Final Reward: 0.92\n"
     ]
    }
   ],
   "source": [
    "is_Terminal = False\n",
    "env.reset()\n",
    "count = 0\n",
    "total_reward = 0\n",
    "\n",
    "print(\"Action\\t\" , \"State\\t\" , \"Reward\\t\" , \"is_Terminal\")\n",
    "\n",
    "while not is_Terminal:\n",
    "    count += 1\n",
    "\n",
    "    rand_action = np.random.choice(4,1)[0]  #0 -> LEFT, 1 -> UP, 2 -> RIGHT, 3 -> DOWN\n",
    "    state, reward, is_Terminal = env.step(rand_action)\n",
    "    \n",
    "    total_reward += reward\n",
    "\n",
    "    print(\"  \", rand_action, \"\\t  \", state, \"\\t\", reward, \"\\t  \", is_Terminal)\n",
    "    \n",
    "print(\"Total Number of steps to Reach Terminal:\", count)\n",
    "print(\"Final Reward:\", total_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The random policy takes large number of steps to reach some terminal state which should be much higher than the number of the steps taken by a all 'Right' policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Right Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action\t State\t Reward\t is_Terminal\n",
      "   2 \t   1 \t -0.01 \t   False\n",
      "   2 \t   2 \t -0.01 \t   False\n",
      "   2 \t   3 \t 1 \t   True\n",
      "Total Number of steps to Reach Terminal: 3\n",
      "Final Reward: 0.98\n"
     ]
    }
   ],
   "source": [
    "is_Terminal = False\n",
    "env.reset()\n",
    "count = 0\n",
    "total_reward = 0\n",
    "\n",
    "print(\"Action\\t\" , \"State\\t\" , \"Reward\\t\" , \"is_Terminal\")\n",
    "\n",
    "while not is_Terminal:\n",
    "    count += 1\n",
    "\n",
    "    right_action = 2  #0 -> LEFT, 1 -> UP, 2 -> RIGHT, 3 -> DOWN\n",
    "    state, reward, is_Terminal = env.step(right_action)\n",
    "    \n",
    "    total_reward += reward\n",
    "\n",
    "    print(\"  \", right_action, \"\\t  \", state, \"\\t\", reward, \"\\t  \", is_Terminal)\n",
    "    \n",
    "print(\"Total Number of steps to Reach Terminal:\", count)\n",
    "print(\"Final Reward:\", total_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The right policy most of the time reaches the goal state in just 3 steps which is expected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1. Find an optimal policy to navigate the given environment using Policy Iteration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization\n",
    "policy = {0: 2, 1: 2, 2: 2, 4: 2, 6: 2, 8: 2, 9: 2, 10: 2, 11: 2}\n",
    "policy_old = {}\n",
    "value_f_old = {0: 1, 1: 1, 2: 1, 3: 0, 4: 1, 5: 0, 6: 1, 7: 0, 8: 1, 9: 1, 10: 1, 11: 1}\n",
    "value_f = value_f_old.copy()\n",
    "# Policy iteration\n",
    "loops_pei = 0  # Number of policy evaluation iteration loops\n",
    "loops_pii = 0  # Number of policy improvement iteration loops\n",
    "gamma =1 #discount_Factor\n",
    "while policy_old != policy:\n",
    "    loops_pii += 1\n",
    "    # Policy Evaluation\n",
    "    tolerance = 1e-6\n",
    "    dif = 1\n",
    "    while dif > tolerance:\n",
    "        loops_pei += 1\n",
    "        for curr_state in policy.keys():\n",
    "            action = policy[curr_state]\n",
    "            value_f[curr_state] = 0\n",
    "            for next_state_info in env.prob_dynamics[curr_state][action]:\n",
    "                prob,next_state, reward, _ = next_state_info\n",
    "                value_f[curr_state] += prob * (reward + gamma*value_f_old[next_state])  # Bellman equation\n",
    "\n",
    "        dif = np.sum(np.abs(np.array([value_f[k] - value_f_old[k] for k in policy.keys()])))\n",
    "        value_f_old = value_f.copy()\n",
    "\n",
    "    # Policy improvement\n",
    "    policy_old = policy.copy()\n",
    "    for curr_state in policy.keys():\n",
    "        avail_actions = list(env.prob_dynamics[curr_state].keys())\n",
    "        max_action_value = -sys.maxsize\n",
    "        best_action = policy[curr_state]\n",
    "        for action in avail_actions:\n",
    "            action_value = 0\n",
    "            for next_state_info in env.prob_dynamics[curr_state][action]:\n",
    "                prob,next_state, reward, _ = next_state_info\n",
    "                action_value += prob * (reward + gamma*value_f[next_state])\n",
    "            if action_value > max_action_value:\n",
    "                max_action_value = action_value\n",
    "                best_action = action\n",
    "        policy[curr_state] = best_action\n",
    "\n",
    "\n",
    "# Convert policy from state indices to action labels\n",
    "policy_labels = {0: 'left', 1: 'up', 2: 'right', 3: 'down'}\n",
    "policy_policy_iteration = {f\"s{i}\": policy_labels[action] for i, action in policy.items()}\n",
    "value_f_policy_iteration = {f\"s{i}\": val for i, val in value_f.items()}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal policy using policy iteration: {'s0': 'right', 's1': 'right', 's2': 'right', 's4': 'up', 's6': 'left', 's8': 'up', 's9': 'left', 's10': 'left', 's11': 'down'}\n",
      "Value function using policy iteration: {'s0': 0.9597242536350085, 's1': 0.9737867558093476, 's2': 0.9862867573277768, 's3': 0, 's4': 0.9472242513565996, 's5': 0, 's6': 0.896580826024145, 's7': 0, 's8': 0.933161748093996, 's9': 0.9206617446752353, 's10': 0.9068749716325266, 's11': 0.8068666069874402}\n",
      "Average number of policy evaluation iteration loops: 100.75\n",
      "Number of policy improvement iteration loops: 4\n"
     ]
    }
   ],
   "source": [
    "print(\"Optimal policy using policy iteration:\", policy_policy_iteration)\n",
    "print(\"Value function using policy iteration:\", value_f_policy_iteration)\n",
    "print(\"Average number of policy evaluation iteration loops:\", loops_pei/loops_pii)\n",
    "print(\"Number of policy improvement iteration loops:\", loops_pii)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2. Find an optimal policy to navigate the given environment using Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization\n",
    "policy = {0: 2, 1: 2, 2: 2, 4: 2, 6: 2, 8: 2, 9: 2, 10: 2, 11: 2}\n",
    "policy_old = {}\n",
    "value_f_old = {0: 1, 1: 1, 2: 1, 3: 0, 4: 1, 5: 0, 6: 1, 7: 0, 8: 1, 9: 1, 10: 1, 11: 1}\n",
    "value_f = value_f_old.copy()\n",
    "\n",
    "# Value iteration\n",
    "max_iteration =1000\n",
    "tolerance = 1e-6\n",
    "dif = 1\n",
    "loops_vi = 0  # Number of value iteration loops\n",
    "gamma=1 #discount_factor\n",
    "\n",
    "while dif > tolerance and loops_vi<max_iteration:\n",
    "\n",
    "    loops_vi += 1\n",
    "    for curr_state in policy.keys():\n",
    "        value_f[curr_state] = 0\n",
    "        for action in range(len(env.prob_dynamics[curr_state])):\n",
    "            action_value = 0\n",
    "            for next_state_info in env.prob_dynamics[curr_state][action]:\n",
    "                prob,next_state, reward, _ = next_state_info\n",
    "                action_value += prob * (reward + gamma*value_f_old[next_state])  # Bellman equation\n",
    "            if action_value > value_f[curr_state]:\n",
    "                value_f[curr_state] = action_value\n",
    "\n",
    "    dif = np.sum(np.abs(np.array([value_f[k] - value_f_old[k] for k in policy.keys()])))\n",
    "    value_f_old = value_f.copy()\n",
    "\n",
    "# Policy improvement\n",
    "for curr_state in policy.keys():\n",
    "    avail_actions = list(env.prob_dynamics[curr_state].keys())\n",
    "    max_action_value = -sys.maxsize\n",
    "    best_action = policy[curr_state]\n",
    "    for action in avail_actions:\n",
    "        action_value = 0\n",
    "        for next_state_info in env.prob_dynamics[curr_state][action]:\n",
    "            prob, next_state, reward, _ = next_state_info\n",
    "            action_value += prob * (reward + gamma*value_f[next_state])\n",
    "        if action_value > max_action_value:\n",
    "            max_action_value = action_value\n",
    "            best_action = action\n",
    "    policy[curr_state] = best_action\n",
    "\n",
    "# Convert policy from state indices to action labels\n",
    "policy_labels = {0: 'left', 1: 'up', 2: 'right', 3: 'down'}\n",
    "policy_value_iteration = {f\"s{i}\": policy_labels[action] for i, action in policy.items()}\n",
    "value_f_value_iteration = {f\"s{i}\": val for i, val in value_f.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal policy using value iteration: {'s0': 'right', 's1': 'right', 's2': 'right', 's4': 'up', 's6': 'left', 's8': 'up', 's9': 'left', 's10': 'left', 's11': 'down'}\n",
      "Value function using value iteration: {'s0': 0.9597242730305761, 's1': 0.9737867713955914, 's2': 0.9862867702538153, 's3': 0, 's4': 0.9472242747438153, 's5': 0, 's6': 0.8965809247091233, 's7': 0, 's8': 0.9331617771971151, 's9': 0.9206617797678368, 's10': 0.9068750213307932, 's11': 0.8068835635255157}\n",
      "Number of value iteration loops: 102\n"
     ]
    }
   ],
   "source": [
    "print(\"Optimal policy using value iteration:\", policy_value_iteration)\n",
    "print(\"Value function using value iteration:\", value_f_value_iteration)\n",
    "print(\"Number of value iteration loops:\", loops_vi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3. Compare PI and VI in terms of convergence. Is the policy obtained by both same?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer: \n",
    "\n",
    "- Both algorithms, Policy Iteration (PI) and Value Iteration (VI), are guaranteed to converge to an optimal policy in the end.\n",
    "\n",
    "- The optimal policy given by both methods is the same.\n",
    "\n",
    "- In terms of convergence, Policy Iteration (PI) involves two main steps: policy evaluation and policy improvement. It iterates between these steps until the policy converges. However, it tends to converge relatively slowly as it alternates between evaluating and improving the policy in each iteration. Policy Iteration requires multiple iterations to converge, and in our case, it took 4 policy improvement loops, with an average of 100.75 policy evaluation iterations in each loop, resulting in a total of 100.75Ã—4=403 iterations.\n",
    "\n",
    "- On the other hand, Value Iteration (VI) directly iterates over the value function and updates it until it converges to the optimal value function. It typically converges faster than PI because it doesn't alternate between policy evaluation and improvement. VI converges after a smaller number of iterations compared to PI. In our case, it took 102 iterations for Value Iteration to converge.\n",
    "\n",
    "- In summary, while both algorithms eventually converge to the optimal policy, Value Iteration tends to converge faster due to its direct iteration over the value function, whereas Policy Iteration alternates between policy evaluation and improvement, resulting in a slower convergence rate."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "4cafede8658e71bdc4b7180bcd658951c639327337cbd78715b7c29dc66075fe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
